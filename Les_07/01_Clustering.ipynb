{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9151bd7",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "Het probleem van supervised learning technieken is dat je steeds het resultaat ter beschikking moet hebben in de vorm van een label of een waarde.\n",
    "Dit houdt in dat je vaak manueel de dataset moet overlopen om deze labelling te doen, bijvoorbeeld over alle beelden bij Computer Vision applicaties.\n",
    "Vaak wordt dit dan door uitbesteed aan meerdere personen en die kunnen af en toe een fout maken waardoor de labelling ook niet altijd perfect is. \n",
    "Dit zorgt voor extra ruis in de dataset die het trainen van een model moeilijker maakt.\n",
    "\n",
    "De tegenhanger van supervised learning is unsupervised learning waar niet-gelabelled data gebruikt wordt voor het uitvoeren van clustering \n",
    "\n",
    "Unsuperivised learning technieken worden gebruikt om te proberen niet-gelabelled data te structureren of te beschrijven.\n",
    "Een aantal veel gebruikte toepassingen van unsuperivsed learning technieken zijn:\n",
    "* Clustering\n",
    "* Anomaly / Outlier Detection\n",
    "* Dimensionality Reduction\n",
    "\n",
    "In deze notebook gaan we focussen op de clustering toepassingen\n",
    "\n",
    "## Clustering\n",
    "\n",
    "In clustering toepassingen worden de beschikbare datapunten gegroepeert in groepen waar de gelijkenissen binnen de groep groter zijn dan tussen groepen.\n",
    "Dit wordt in veel veschillende toepassingen gebruikt zoals \n",
    "* Gezichtsherkenning\n",
    "* Gerelateerde zoekresultaten/producten\n",
    "* Verdelen van je doelpubliek voor personalisering van reclame.\n",
    "* Social Media Clustering om pagina's aan te raden die je zouden interesseren.\n",
    "* Image segmentation\n",
    "\n",
    "Meer informatie over de beschikbare implementaties van clustering algoritmes vind je [hier](https://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e18fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# graphical\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe755c",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "Het meest gekende algoritme voor clustering uit te voeren is het K-Means algoritme.\n",
    "Dit algoritme vereist dat je zelf het aantal clusters kiest dat je verwacht in de dataset.\n",
    "Hoe veel clusters zie je in onderstaande voorbeeld?\n",
    "Hoe heb je dit bepaald?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09260f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, \n",
    "                                    n_classes= 2, n_clusters_per_class=2, class_sep=3, random_state=200)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce8cbb",
   "metadata": {},
   "source": [
    "We zagen deze clusters intuitief. \n",
    "Dit wordt echter zeer moelijk wanneer het aantal features/dimensies toeneemt.\n",
    "Nu gaan we overlopen hoe het K-means clustering algoritme deze clustering uitvoert.\n",
    "Het algoritme bestaat uit de volgende stappen:\n",
    "* stap 1: Neem K willekeurige punten. Deze vormen de centrums van de beschikbare clusters en worden de centroids genoemd.\n",
    "* stap 2: verdeel de datapunten in clusters op basis van de kleinste afstand tot de centroids.\n",
    "* stap 3: verplaats de centroids naar het gemiddelde van alle punten in de cluster.\n",
    "* stap 4: Herhaal stap 2 en 3 tot er geen verandering meer is.\n",
    "\n",
    "**Merk op dat de schaal van de features belangrijk is omdat er met een afstand gewerkt wordt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18590d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=4\n",
    "colors=['red','green','blue','orange']\n",
    "X_max = np.max(X, axis=0)\n",
    "X_min = np.min(X, axis=0)\n",
    "\n",
    "# stap 1\n",
    "np.random.seed(10) \n",
    "centroids = np.random.random(size=(4,2))\n",
    "display(X_min)\n",
    "display(centroids)\n",
    "centroids = centroids * (X_max-X_min) + X_min\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], alpha=0.5)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stap 2\n",
    "def calc_cluster(row):\n",
    "    min_dist = 100000000\n",
    "    best_cluster = -1\n",
    "    for i in np.arange(K):\n",
    "        dist = (row[0] - centroids[i, 0])**2 + (row[1] - centroids[i, 1])**2\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_cluster = i\n",
    "    \n",
    "    return best_cluster\n",
    "    \n",
    "\n",
    "clusters = np.apply_along_axis(calc_cluster, axis=1, arr=X )\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], alpha=0.5, hue=clusters, palette=colors)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07921dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stap 3\n",
    "def calc_new_centroids(X, centroids, clusters):\n",
    "    new_centroids = np.zeros(shape=centroids.shape)\n",
    "    for i in np.arange(K):\n",
    "        if np.sum(clusters ==i) > 0:\n",
    "            new_centroids[i] = X[clusters ==i].mean(axis=0)\n",
    "    return new_centroids\n",
    "\n",
    "new_centroids = calc_new_centroids(X, centroids, clusters)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], alpha=0.2, hue=clusters, palette=colors)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], color='gray')\n",
    "plt.scatter(new_centroids[:,0], new_centroids[:,1], color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stap 4\n",
    "iterations = 0\n",
    "while (centroids != new_centroids).all():\n",
    "    centroids = new_centroids\n",
    "    clusters = np.apply_along_axis(calc_cluster, axis=1, arr=X )\n",
    "    new_centroids = calc_new_centroids(X, centroids, clusters)\n",
    "    \n",
    "    iterations+= 1\n",
    "    \n",
    "print(\"Number of iterations:\", iterations)\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], alpha=0.5, hue=clusters, palette=colors)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4898b44",
   "metadata": {},
   "source": [
    "Dit was de manuele manier om dit uit te voeren.\n",
    "Gelukkig kan dit ook gedaan worden met sklearn door gebruik te maken van de [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html?highlight=kmeans#sklearn.cluster.KMeans) klasse.\n",
    "\n",
    "Via bovenstaande manier is het duidelijk dat het KMeans algoritm gemakkelijke te implementeren en interpreteren is.\n",
    "Er zijn echter een aantal situaties waar er extra aandacht vereist is.\n",
    "Het is echter niet altijd gegarandeerd dat het algoritme de optimale oplossing geeft.\n",
    "Daarnaast kan de oplossing ook beinvloed worden door de beginsituatie.\n",
    "Om de impact hiervan te minimaliseren kan je voor elke combinatie van hyperparameters een aantal modellen laten genereren.\n",
    "Een andere oplossing is om de punten niet random te kiezen maar de centroids zo veel mogelijk te spreiden.\n",
    "Een derde oplossing is om de centroids te plaatsen op punten van de dataset.\n",
    "\n",
    "Ook is het model gevoelig aan outliers omdat deze zogezegd tot de andere cluster dan gaan horen.\n",
    "Het gebruik van een eenvoudige afstand heeft ook zijn probleem situaties.\n",
    "Dit komt overeen met situaties van SVM waar de data niet lineair scheidbaar was.\n",
    "De oplossing hiervan is hetzelfde.\n",
    "Je kan namelijk een kernel gebruiken die de punten projecteert in een hogere dimensie en waar de afstand dan wel gebruikt kan worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ff26e79",
   "metadata": {},
   "source": [
    "In het voorbeeld dat we hier uitgewerkt hebben konden we op het zicht bepalen dat er 4 of 2 clusters gingen zijn.\n",
    "Dit gaat echter niet altijd mogelijk zijn wanneer het aantal features/dimensies toeneemt.\n",
    "Het optimale aantal clusters kan op een aantal manieren bepaald worden:\n",
    "* Silhouette clustering\n",
    "* Elbow method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acdf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb30b0b",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "Bij hierarchical clustering wordt er getracht om een clusterstructuur op te bouwen. Dit wil dus zeggen dat je voor elk aantal clusters een bepaalde verdeling van de dataset bekomt.\n",
    "De methoden voor deze vorm van clustering kunnen in twee verdeeld worden. Namelijk \n",
    "* Agglomeratief of bottom-up\n",
    "* Divise of top-down\n",
    "\n",
    "De voordelen van hierarchical clustering is dat je op elk moment kan stoppen in het algoritme en dus zo elk gewenst aantal clusters kan bekomen.\n",
    "Ook kan de resulterende structuur van clusters handig zijn, bijvoorbeeld voor productcategorieen samen te stellen of in de biologie.\n",
    "\n",
    "### Agglomerative Hierarchical clustering\n",
    "\n",
    "Bij agglomeratieve methoden wordt er begonnen van alle datapunten. \n",
    "De twee dichtste punten worden samengevoegd tot twee clusters. \n",
    "Daarna worden de volgende twee dichtste punten\\clusters samengevoegd tot 1 clusters. \n",
    "Dit proces gaat door tot er nog maar 1 cluster overblijft.\n",
    "Hierbij is het belangrijk om op te merken dat er verschillende metrieken mogelijk zijn om de afstand tussen twee clusters te bepalen, bijvoorbeeld:\n",
    "* maximum afstand tussen een punt uit elke cluster\n",
    "* minimum afstand tussen een punt uit elke cluster\n",
    "* de gemiddelde afstand tussen elk punt uit de ene cluster en elk punt uit de andere cluster\n",
    "* ...\n",
    "\n",
    "Merk op dat je voor deze metrieken te berekenen steeds elk element van een cluster moet vergelijken met elk element van een andere cluster.\n",
    "Dit wordt dus snel zeer rekenintensief, minstens kwadratisch met het aantal datapunten.\n",
    "\n",
    "### Divisive hierarchical clustering\n",
    "\n",
    "Bij dit type van hierarchical clustering vertrekken we van 1 cluster en zoeken we naar de beste manier om deze cluster te verdelen in twee.\n",
    "Daarna doen we hetzelfde voor elk van de twee volgende clusters en zo verder tot er slechts 1 element in elke cluster aanwezig is.\n",
    "Omdat er zoveel mogelijke manieren zijn om elke cluster te splitsen worden er gebruik gemaakt van heuristieken in deze methode.\n",
    "Het basis algoritme hiervan wordt DIANA genoemd maar we gaan hier niet dieper op in.\n",
    "\n",
    "## Mean-Shift Clustering\n",
    "\n",
    "Een laatste manier die ik aanhaal voor clustering uit te voeren is de mean-shift clustering.\n",
    "Dit is een algoritme dat rechtstreeks op zoek gaat naar het middelpunt van elke cluster door het middelpunt te verplaatsen naar het middelpunt van alle datapunten binnen een bepaalde afstand van het huidige middelpunt (centroid).\n",
    "Meer informatie en een animatie van hoe dit algoritme in zijn werk gaat kan je [hier](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68) vinden.\n",
    "Het algoritme bestaat uit de volgende stappen:\n",
    "* Neem x aantal punten uniform verdeeld over de featureset\n",
    "* Voor elk punt, zoek naar alle datapunten die vallen binnen een straal R (**het sliding window**) en bereken het middelpunt\n",
    "* Verplaats de punten naar hun middelpunt en herhaal het vorige tot er convergentie is.\n",
    "* Wanneer het sliding window van verschillende centroids overlapt (afstand kleiner dan 2R), dan blijft enkel het punt met het hoogste aantal punten binnen zijn sliding window over\n",
    "* Er blijven K aantal punten over die de centers zijn van de clusters\n",
    "\n",
    "Het grote voordeel van deze techniek is dat er geen nood is om te optimaliseren op het aantal clusters aangezien dit door het algoritme bepaald wordt.\n",
    "Wel speelt de straal R van het sliding window een grote rol en kan een grote impact hebben op het aantal en de locatie van de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6456f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e378fa3c",
   "metadata": {},
   "source": [
    "# Oefening\n",
    "\n",
    "Voer clustering uit voor de dataset die je [hier](https://www.kaggle.com/hellbuoy/online-retail-customer-clustering) kan vinden.\n",
    "* Verwijder rijen die null waarden bevatten\n",
    "* Zet de dataset om naar een dataset die per klant bijhoudt van welk land de klant afkomstig is (origin), aantal keer dat de klant iets gekocht heeft (numPurchases), totaal aantal bedrag gekocht (TotalAmount), tijd sinds laatste aankoop (TimePassed) \n",
    "* Voer K-Means clustering uit voor een zelf gekozen aantal clusters (minstens 3) op deze dataset en bereken de score.\n",
    "* Merk op dat de schaal van de bovenstaande kolommen niet consistent is. Zet alle kolommen op dezelfde schaal door gebruik te maken van de StandardScaler.\n",
    "* Voer opnieuw K-Means clustering uit met hetzelfde aantal clusters als hierboven op deze aangepaste dataset en vergelijk de score met de vorige. Verklaar het resultaat.\n",
    "* Ga nu op zoek naar het optimaal aantal clusters door middel van de elbow methode\n",
    "* Voer nu ook clustering uit door middel van Mean Shift Clustering. Is het aantal clusters bekomen met deze techniek gelijkaardig aan het beste aantal voor K-Means.\n",
    "* Print nu een twintig-tal aantal rijen uit met hun labels (deze bekom je via: model.labels_). Kan je visueel zien welke parameters gebruikt zijn voor de clustering? Verklaar je antwoord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888053b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
