{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7cb2d3",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines of SVM is een supervised machine learning techniek die heel populair is omdat het het kan omgaan met heel veel features en weinig data en toch goede resultaten bekomen.\n",
    "Ook in het geval van heel veel data heeft deze techniek voordelen, namelijk omdat ze heel geheugenefficient is omdat er maar een beperkt deel van de data gebruikt wordt voor het model te trainen.\n",
    "Deze techniek kan gebruikt worden voor regressie toe te passen maar is veel populairder als classifier.\n",
    "SVM als classifier is een **binaire classificatie** techniek maar kan omgaan met multi-class problemen door gebruik te maken van de One-Vs-All of One-Vs-One methoden. Standaard wordt er One-vs-One gebruikt voor multi-class toepassingen\n",
    "\n",
    "## Werking\n",
    "\n",
    "Op basis van de trainingsexamples wordt gezocht naar een zo goed mogelijke **lineaire** scheiding van de trainingsexamples.\n",
    "Hierbij wordt de marge tot de dichtste punten van de verschillende klassen zo groot mogelijk gemaakt. \n",
    "Bij het plaatsen van de beste lineaire scheiding worden enkel de punten die het dichtst bij de scheiding liggen gebruikt.\n",
    "Deze observaties worden de **support vectors** gebruikt en enkel deze punten worden gebruikt omdat verder gelegen punten geen informatie bevatten over de scheidingslijn. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16804d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc877e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_decision_boundary(x, y, model, levels=True, color='green'):\n",
    "    # Visualiseer de decision boundary (gaat enkel werken bij 2 features)\n",
    "\n",
    "    h = 0.01\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    y_min = y.min()\n",
    "    y_max = y.max()\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    \n",
    "    if levels:\n",
    "        xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        predicts = model.decision_function(xy).reshape(xx.shape)\n",
    "        plt.contour(xx, yy, predicts, colors=color, levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    else:\n",
    "        predicts = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        predicts = predicts.reshape(xx.shape)\n",
    "        plt.contour(xx, yy, predicts, colors=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1769c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=20, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, class_sep = 1.5, random_state=98765)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891393a",
   "metadata": {},
   "source": [
    "Wat is nu de beste scheidingslijn die we kunnen tekenen hier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "plt.plot([-1.9, 3], [-0.3,3])\n",
    "plt.plot([-0.8, 0.2], [0,3])\n",
    "plt.plot([-1.5, -0.5], [0,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d14b95",
   "metadata": {},
   "source": [
    "Intuitief kiezen we de lijn met de grootste marge als beste lijn.\n",
    "Deze marge maximaliseren is net hetgene dat er gebeurd bij SVM. \n",
    "In het voorbeeld hieronder wordt de lijn getekend die SVM uitkomt voor deze dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7e29e",
   "metadata": {},
   "source": [
    "## Verschil met logistische regressie\n",
    "\n",
    "SVM gaat de marge tussen de scheidingslijn en de support vectors gaan maximaliseren.\n",
    "Dit werkt anders dan het minimaliseren van de kost-functie waar we kunnen naar de fout van alle punten.\n",
    "Dit kan zorgen voor een andere gewichten.\n",
    "Ook zijn er nog de volgende verschillen:\n",
    "* Logistische regressie werkt goed voor onafhankelijke variabelen en SVM werkt op semi- of ongestructureerde data zoals tekst of beelden.\n",
    "* SVM maakt gebruik van geometrische informatie tussen de verschillende datapunten terwijl logistische regressie eerder gebruik maakt van statistische eigenschappen.\n",
    "* SVM heeft een lagere kans op overfitting dan logistische regressie\n",
    "* SVM is gevoeliger voor outliers dan logistische regressie\n",
    "* SVM geeft geen indicatie over de zekerheid waarmee een observatie tot een klasse behoort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(X, y)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], modelLR, levels=False, color='red')\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5369800",
   "metadata": {},
   "source": [
    "### Wat als strikt lineaire scheiding niet mogelijk is?\n",
    "\n",
    "In veel situaties kunnen de observaties niet altijd 100% gesplitst worden in twee klassen en zal er een zone zijn waar er overlap mogelijk is tussen de klassen.\n",
    "Om dan nog steeds met SVM te werken kunnen we gebruik maken van de regularisatie-parameter **C**.\n",
    "Deze verzacht de constraint van strikte splitsing en laat overlap van de klassen toe. \n",
    "De betekenis van deze C-parameter is:\n",
    "* Grote C zorgt ervoor dat er een kleine toegelaten marge is waarin naar support vectors gezocht wordt. Een grotere waarde verhoogt de kans op overfitting.\n",
    "* Lagere C waarde laat een grotere marge toe en dus worden er meer support vectors gekozen. Dit kan leiden tot underfitting omdat er te veel vectors een impact hebben op het eindresultaat.\n",
    "\n",
    "Kans opvragen waarmee het tot een klasse hoort gaat niet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024105cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=20, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, class_sep = 1, random_state=1111)\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "\n",
    "model = SVC(kernel='linear', C=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ac801",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "\n",
    "model = SVC(kernel='linear', C=10000)\n",
    "model.fit(X, y)\n",
    "\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f993a6",
   "metadata": {},
   "source": [
    "## Wat als de datapunten totaal niet lineair scheidbaar zijn?\n",
    "\n",
    "Hoe zou je onderstaande dataset splitsen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7491e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(noise=0.1, random_state=1111, factor=0.5)\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff5abb",
   "metadata": {},
   "source": [
    "Het is duidelijk dat er geen enkele rechte te tekenen is op deze scatter plot die de datapunten in twee deelt.\n",
    "Bij logistische regressie hebben we om dit probleem op te lossen hogere-orde features toegevoegd.\n",
    "Dit kunnen we echter niet doen bij SVM.\n",
    "Om dit probleem op te lossen moeten we hier onze datapunten projecteren naar een hogere dimensie.\n",
    "Een visualisatie van hoe dit werkt kan je vinden in [deze video](https://www.youtube.com/watch?v=3liCbRZPrZA).\n",
    "Een voorbeeld van hoe je dit kan doen in het bovenstaande is om de afstand tot de oorsprong te berekenen van elk punt.\n",
    "Hierdoor gaan in deze extra derde dimensie/feature alle blauwe waarden een grotere afstand hebben.\n",
    "Op basis van deze extra dimensie kan dan een vlak getekend worden dat een lineaire scheiding verzorgd tussen de twee klassen.\n",
    "De functie die deze projectie verzorgt naar een hogere dimensie wordt ook een **kernel** genoemd.\n",
    "Een heel aantal functies kan gebruikt worden als kernel maar de meest gebruikte zijn:\n",
    "* Lineaire kernel (geen hogere dimensies gebruikt)\n",
    "* Gaussiaanse kernel of Radial Basis function (RBF), dit is de default waarde bij sklearn.svm. Bij gebruik van deze kernel moet je er voor zorgen dat eerst normalisatie toegepast wordt.\n",
    "* Polynomial kernel (Vooral populair bij Natural Language Processing applicaties en beperkt tot graad 2 omdat er anders eens sterke neiging is tot overfitting)\n",
    "\n",
    "Indien er een niet-lineaire kernel gebruikt wordt, wordt er ook een extra hyperparameter geintroduceerd.\n",
    "Deze parameter wordt $\\gamma$ genoemd en geeft de breedte van de kernel weer of hoe snel de extra dimensie varieert voor aanpassingen van de features.\n",
    "Algemeen gezien kan je stellen dat:\n",
    "* Kleine gamma leidt tot brede / traag variërende kernel wat tot underfitting kan leiden indien de gamma te klein is\n",
    "* Grote gamma leidt tot snel variërende kernels wat kan leiden tot overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed653472",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "model.fit(X, y)\n",
    "\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "\n",
    "model = SVC(kernel='rbf', gamma=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "visualise_decision_boundary(X[:,0], X[:, 1], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf1cdb",
   "metadata": {},
   "source": [
    "### Wanneer welke classifier kiezen\n",
    "\n",
    "* Aantal features groot tov aantal examples -> SVM met lineaire kernel of logistic regression \n",
    "* Klein aantal features en redelijk aantal examples -> SVM met gaussian kernel \n",
    "* Klein aantal features en heel veel examples -> Logistic Regression met hogere-orde features of SVM met lineaire kernel\n",
    "\n",
    "### Evaluatie van de SVM-model\n",
    "\n",
    "De evaluatie van het resulterende model kan berekend worden net zoals bij logistische regressie, namelijk door gebruik te maken van F1-score per klasse of de Micro/Macro/Weighted F1-score voor een globaal resultaat.\n",
    "Ook is de accuraatheid steeds een goed begin.\n",
    "\n",
    "### Oefening:\n",
    "\n",
    "Gebaseerd op de [iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) dataset, maak een SVM en Logistic Regression classifier en zoek naar de classifier met de hoogste accuraatheid.\n",
    "\n",
    "* Geef een histogram, boxplot en correlatie matrix van de features in de dataset\n",
    "* Verdeel de data in trainings- en testdata\n",
    "* Train de modellen\n",
    "* Evalueer de classifiers door de verschillende metrieken te berekenen per klasse en globaal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb3f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
